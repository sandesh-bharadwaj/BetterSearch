from transformers import BitsAndBytesConfig
import datetime
from .util import clean_sqlcoder_output, get_file_indexer, get_prompt_format, get_model_and_tokenizer, get_table_info, validate_correct_sql_query
from pathlib import Path
import os
from ..database.constants import parsable_exts


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        
class BetterSearchPipeline:
    def __init__(self, model_name: str = None, cache_dir: str = None, 
                 bnb_config: BitsAndBytesConfig = None, kv_cache_flag: bool = True, 
                 num_beams: int = 4, db_path: str = "better_search_content_db", embd_model_device: str = "cuda", **kwargs) -> None:
        """
        Initialize the pipeline with the given parameters.

        Args:
            model_name (str): Name of the model to be used.
            cache_dir (str): Directory to cache the model.
            bnb_config (BitsAndBytesConfig): Configuration for bits and bytes.
            kv_cache_flag (bool): Flag to enable or disable key-value caching.
            num_beams (int): Number of beams for beam search.
            db_path (str): Path to the vector database.
            embd_model_device (str): Device to run the embedding model on.
            **kwargs: Additional keyword arguments.
        """
        self.file_indexer = get_file_indexer(db_path=db_path, device=embd_model_device, **kwargs)
        self.model, self.tokenizer = get_model_and_tokenizer(model_name, cache_dir, bnb_config, kv_cache_flag, **kwargs)
        self.num_beams = num_beams
        self.sqlPrompt_format = get_prompt_format(Path(BASE_DIR,"./sqlcoder_prompt.md"))
        self.llamaPrompt_format = get_prompt_format(Path(BASE_DIR,"./llama_prompt.md"))
        self.table_metadata_string, self.table_name = get_table_info()
        self.file_formats = {k: ", ".join(str(x) for x in v) for k,v in parsable_exts.items()}
        self.history = []
    
    def answer(self, user_question):
        """
        Generate an answer to the user's question using the LLM and the vector database.

        Args:
            user_question (str): The question posed by the user.

        Returns:
            str: The answer generated by the LLM.
        """
        # First step: Initial prompt to LLM generates an SQL query
        curr_prompt = self.sqlPrompt_format.format(
            user_question=user_question, 
            table_metadata_string=self.table_metadata_string, 
            date_time=datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            )
        
        # Generate the SQL query
        output = self.tokenizer.batch_decode(
            self.model.generate(
                **self.tokenizer(
                    curr_prompt, return_tensors="pt"
                ),
                num_return_sequences=1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
                max_new_tokens=400,
                do_sample=False,
                num_beams=self.num_beams
            ),
            skip_special_tokens=True
        )[0]
        
        # Clean and validate the generated SQL query
        output = clean_sqlcoder_output(output, self.table_metadata_string, self.table_name)
        output = validate_correct_sql_query(output)
        
        # Second step: Use user_context (SQL query output or content search) to get the final answer.
        user_context = self.file_indexer.query(output, user_question)
        curr_prompt = self.llamaPrompt_format.format(
            user_question=user_question,
            user_context=user_context
        )
        
        # Generate the final answer using the user context
        output = self.tokenizer.batch_decode(
            self.model.generate(
                **self.tokenizer(
                    curr_prompt, return_tensors="pt"
                ),
                num_return_sequences=1,
                max_new_tokens=400,
                eos_token_id=[self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids("<|eot_id|>")],
                pad_token_id=self.tokenizer.eos_token_id,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
            ),
            skip_special_tokens=True
        )[0].split("```Ans:")[-1].strip()
        
        # Clean the final output
        # Probably should do comprehensive cleanup for assistant answers, we'll see how things go
        output = output.replace('```','')
        
        return output
        
        
        
        
        
        